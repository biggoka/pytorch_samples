{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from itertools import count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, capacity=10000, short_capacity=2000):\n",
    "        self.capacity = capacity\n",
    "        self.short_capacity = short_capacity\n",
    "        self.reset()\n",
    "        \n",
    "    def push(self, old_state, action, reward, new_state, final):\n",
    "        self.short_memory.append([old_state, action, reward, new_state, final])\n",
    "        \n",
    "    def commit(self, final=True): \n",
    "        self.short_memory[-1][-1] = final\n",
    "        self.memory.extendleft(self.short_memory)\n",
    "        self.short_memory.clear()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "        self.short_memory = deque(maxlen=self.short_capacity)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = deque(maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([4, 3, 2, 1])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.extendleft([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Memory():\n",
    "#     def __init__(self, capacity=10000):\n",
    "#         self.capacity = capacity\n",
    "#         self.memory = []\n",
    "#         self.short_memory = []\n",
    "        \n",
    "#     def push(self, old_state, action, reward, new_state, final):\n",
    "#         self.short_memory.append([old_state, action, reward, new_state, final])\n",
    "        \n",
    "#     def commit(self, final=True): \n",
    "#         if len(self.memory) + len(self.short_memory) < self.capacity:\n",
    "#             self.memory.extend(self.short_memory)\n",
    "#         else:\n",
    "#             for memo in self.short_memory:\n",
    "#                 self.memory.insert(random.randint(0, len(self.memory) - 1), memo)\n",
    "                \n",
    "#         self.memory = self.memory[:self.capacity]\n",
    "#         self.short_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 32),\n",
    "#             torch.nn.LeakyReLU(inplace=True),\n",
    "#             torch.nn.Linear(32, 32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(32, 2),\n",
    "#             torch.nn.Sigmoid(),\n",
    "        #     torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, observation_space=None, action_space=None, device=None):\n",
    "        self.device = device\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self._mode = 'train'\n",
    "        self._episode_count = 0\n",
    "        self._eps = 0.99999\n",
    "        self._eps_min = 0.01\n",
    "        self._gamma = 0.9995\n",
    "        \n",
    "        self.policy_net = Qnet().to(self.device)\n",
    "        self.target_net = Qnet().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optim = torch.optim.Adam(self.policy_net.parameters(), lr=0.05)\n",
    "        \n",
    "        self.memory = Memory(capacity=10000000)  \n",
    "    \n",
    "    def push_observation(self, old_state, action, reward, state, final):\n",
    "        self.memory.push(old_state, action, reward, state, final)\n",
    "    \n",
    "    def episode_ended(self):\n",
    "        self.memory.commit()\n",
    "        self._episode_count += 1\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        def get_model_action(state):\n",
    "            with torch.no_grad():\n",
    "                input = torch.FloatTensor(state).to(self.device)\n",
    "                model_output = self.policy_net(input.unsqueeze(0))\n",
    "                model_output = model_output.cpu().detach().numpy()\n",
    "                return np.argmax(model_output[0])\n",
    "            \n",
    "        if self._mode == 'eval':\n",
    "            return get_model_action(state)\n",
    "        if self._mode == 'train':\n",
    "            self._eps *= self._gamma\n",
    "            self._eps = max(self._eps, self._eps_min)\n",
    "            if random.random() > self._eps:\n",
    "                return get_model_action(state)\n",
    "            else:\n",
    "                return random.randint(0, 1)\n",
    "        raise \"unknown mode\"\n",
    "        \n",
    "    def train_memory(self):\n",
    "        memory = self.memory.memory\n",
    "        try:\n",
    "            memory = random.sample(memory, 512)\n",
    "        except ValueError:\n",
    "            return\n",
    "\n",
    "        s0 = [x[0] for x in memory]\n",
    "        a = [x[1] for x in memory]\n",
    "        r = [x[2] for x in memory]\n",
    "        s1 = [x[3] for x in memory]\n",
    "        done = [1. if x[4] else 0. for x in memory]\n",
    "\n",
    "\n",
    "        s0 = torch.FloatTensor(s0).to(self.device)\n",
    "        s1 = torch.FloatTensor(s1).to(self.device)\n",
    "        a = torch.LongTensor(a).to(self.device)\n",
    "        r = torch.FloatTensor(r).to(self.device)\n",
    "        done = torch.FloatTensor(done).to(self.device)\n",
    "\n",
    "        q_values = self.policy_net(s0)\n",
    "        next_q_values = self.target_net(s1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "\n",
    "        q_value = q_values.gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "        expected_q_value = r + 0.999 * next_q_value * (1.0 - done)\n",
    "        # Notice that detach the expected_q_value\n",
    "        loss = (expected_q_value.detach() - q_value).pow(2).mean()\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1)\n",
    "        self.optim.step()\n",
    "        \n",
    "        def soft_update(target_model, source_model, tau=0.001):\n",
    "            for target_param, source_param in zip(target_model.parameters(), source_model.parameters()):\n",
    "                target_param.data.copy_(tau*source_param.data + (1.0-tau)*target_param.data)\n",
    "                \n",
    "        soft_update(self.target_net, self.policy_net)\n",
    "        \n",
    "        \n",
    "#         if self._episode_count % 10 == 0:\n",
    "#             self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "  \n",
    "    def train(self):\n",
    "        self._mode = 'train'\n",
    "    def eval(self):\n",
    "        self._mode = 'eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCriticNet, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "#             torch.nn.BatchNorm1d(4),\n",
    "            torch.nn.Linear(4, 64),\n",
    "#             torch.nn.LeakyReLU(inplace=True),\n",
    "#             torch.nn.Linear(32, 32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "#             torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "#             torch.nn.BatchNorm1d(64),\n",
    "#             torch.nn.Sigmoid(),\n",
    "        #     torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "        self.critic = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(64, 64),\n",
    "#             torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(64, 64),\n",
    "#             torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(64, 2),\n",
    "#             torch.nn.LeakyReLU(inplace=True),\n",
    "#             torch.nn.LogSoftmax(dim=1),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "#             torch.nn.functional.softmax(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = self.model(x)\n",
    "        return self.critic(res), self.actor(res)\n",
    "    \n",
    "# def calc_actual_state_values(states, rewards, dones):\n",
    "#     R = []\n",
    "#     rewards.reverse()\n",
    "\n",
    "#     # If we happen to end the set on a terminal state, set next return to zero\n",
    "#     if dones[-1] == True: next_return = 0\n",
    "        \n",
    "#     # If not terminal state, bootstrap v(s) using our critic\n",
    "#     # TODO: don't need to estimate again, just take from last value of v(s) estimates\n",
    "#     else: \n",
    "#         s = torch.from_numpy(states[-1]).float().unsqueeze(0)\n",
    "#         next_return = model.get_state_value(Variable(s)).data[0][0] \n",
    "    \n",
    "#     # Backup from last state to calculate \"true\" returns for each state in the set\n",
    "#     R.append(next_return)\n",
    "#     dones.reverse()\n",
    "#     for r in range(1, len(rewards)):\n",
    "#         if not dones[r]: this_return = rewards[r] + next_return * GAMMA\n",
    "#         else: this_return = 0\n",
    "#         R.append(this_return)\n",
    "#         next_return = this_return\n",
    "\n",
    "#     R.reverse()\n",
    "#     state_values_true = Variable(torch.FloatTensor(R)).unsqueeze(1)\n",
    "    \n",
    "#     return state_values_true\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self, observation_space=None, action_space=None, device=None):\n",
    "        self.device = device\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self._mode = 'train'\n",
    "        self._episode_count = 0\n",
    "        self._eps = 0.99999\n",
    "        self._eps_min = 0.01\n",
    "        self._gamma = 0.9995\n",
    "        \n",
    "        self.policy_net = ActorCriticNet().to(self.device)\n",
    "        self.target_net = ActorCriticNet().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optim = torch.optim.Adam(self.policy_net.parameters(), lr=0.003)\n",
    "        \n",
    "        self.memory = Memory(capacity=20000)  \n",
    "    \n",
    "    def push_observation(self, old_state, action, reward, state, final):\n",
    "        self.memory.push(old_state, action, reward, state, final)\n",
    "    \n",
    "    def episode_ended(self):\n",
    "        self.memory.commit()\n",
    "        self._episode_count += 1\n",
    "        \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        self.target_net.eval()\n",
    "        self.policy_net.eval()\n",
    "        with torch.no_grad():\n",
    "            input = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "#             sv, action_probs = self.target_net(input)\n",
    "            sv, action_probs = self.policy_net(input)\n",
    "#             action_probs = torch.nn.functional.softmax(action_probs_v)\n",
    "#             print(input, sv, action_probs_v, action_probs)\n",
    "        \n",
    "        if deterministic == False:\n",
    "            return action_probs.multinomial(1).data[0][0].item()\n",
    "        else:\n",
    "#             assert False\n",
    "            return action_probs.max(1)[1].data[0].item()\n",
    "        \n",
    "        \n",
    "#         if self._mode == 'eval':\n",
    "#             return get_model_action(state)\n",
    "#         if self._mode == 'train':\n",
    "#             self._eps *= self._gamma\n",
    "#             self._eps = max(self._eps, self._eps_min)\n",
    "#             if random.random() > self._eps:\n",
    "#                 return get_model_action(state)\n",
    "#             else:\n",
    "#                 return random.randint(0, 1)\n",
    "#         raise \"unknown mode\"\n",
    "        \n",
    "    def train_memory(self):\n",
    "        self.policy_net.train()\n",
    "#         self.policy_net.load_state_dict(self.target_net.state_dict())\n",
    "        \n",
    "        memory = self.memory.memory\n",
    "#         try:\n",
    "#             memory = random.sample(memory, 128)\n",
    "#         except ValueError:\n",
    "#             return\n",
    "\n",
    "        s0 = [x[0] for x in memory]\n",
    "        a = [x[1] for x in memory]\n",
    "        r = [x[2] for x in memory]\n",
    "        s1 = [x[3] for x in memory]\n",
    "        done = [1. if x[4] else 0. for x in memory]\n",
    "\n",
    "\n",
    "        s0 = torch.FloatTensor(s0).to(self.device)\n",
    "        s1 = torch.FloatTensor(s1).to(self.device)\n",
    "        a = torch.LongTensor(a).to(self.device).view(-1, 1)\n",
    "        r = torch.FloatTensor(r).to(self.device)\n",
    "        done = torch.FloatTensor(done).to(self.device)\n",
    "        \n",
    "        s0_values, action_probs = self.policy_net(s0)\n",
    "        action_log_probs = action_probs.log()\n",
    "        chosen_action_log_probs = action_log_probs.gather(1, a)\n",
    "        \n",
    "        s1_values = self.policy_net(s1)[0] * 0.9999 + r# + s0_values\n",
    "#         expected_s1_values = s1_values * 0.99 * (1-done) #+ r\n",
    "        \n",
    "#         r_l = list(r.numpy())\n",
    "#         R = [0]\n",
    "#         for rr in r_l[1:]:\n",
    "#             R.append(R[-1]*0.95 + rr)\n",
    "#         print(R, r_l)\n",
    "#         assert False\n",
    "#         true_state_values = torch.FloatTensor(R).to(self.device).unsqueeze(0)\n",
    "        \n",
    "        \n",
    "        advantages = s1_values - s0_values\n",
    "        \n",
    "        entropy = (action_probs * action_log_probs).sum(1).mean()\n",
    "        action_gain = (chosen_action_log_probs * advantages).mean()\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "        total_loss = value_loss*0.8 - action_gain - 0.0001*entropy\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1)\n",
    "        self.optim.step()\n",
    "\n",
    "#         q_values = self.policy_net(s0)\n",
    "#         next_q_values = self.target_net(s1)\n",
    "#         next_q_value = next_q_values.max(1)[0]\n",
    "\n",
    "#         q_value = q_values.gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "#         expected_q_value = r + 0.999 * next_q_value * (1.0 - done)\n",
    "#         # Notice that detach the expected_q_value\n",
    "#         loss = (expected_q_value.detach() - q_value).pow(2).mean()\n",
    "\n",
    "#         self.optim.zero_grad()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1)\n",
    "#         self.optim.step()\n",
    "        \n",
    "#         def soft_update(target_model, source_model, tau=0.05):\n",
    "#             for target_param, source_param in zip(target_model.parameters(), source_model.parameters()):\n",
    "#                 target_param.data.copy_(tau*source_param.data + (1.0-tau)*target_param.data)\n",
    "                \n",
    "#         soft_update(self.target_net, self.policy_net)\n",
    "        \n",
    "        \n",
    "#         if self._episode_count % 10 == 0:\n",
    "#             self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "  \n",
    "    def train(self):\n",
    "        self._mode = 'train'\n",
    "    def eval(self):\n",
    "        self._mode = 'eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 952,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCritic(observation_space=4, action_space=2, device=device)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   0:     11.000 iterations, eps=0.999990\n",
      "Episode 100:     64.220 iterations, eps=0.999990\n",
      "Episode 200:    190.900 iterations, eps=0.999990\n"
     ]
    }
   ],
   "source": [
    "len_mem = []\n",
    "try:\n",
    "    for episode in range(100000):\n",
    "        state = env.reset()\n",
    "\n",
    "        for i in count():\n",
    "#             env.render()\n",
    "\n",
    "#             action = get_model_action(policy_net, state, device)\n",
    "            action = agent.get_action(state)\n",
    "            old_state = state\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                env.close()\n",
    "                len_mem.append(i)\n",
    "                \n",
    "                if abs(i-2000) < 10:\n",
    "                    reward = 1\n",
    "                    final = False\n",
    "                else:\n",
    "                    reward = -1\n",
    "                    final = True\n",
    "                \n",
    "                agent.push_observation(old_state, action, reward, state, final)\n",
    "                agent.episode_ended()\n",
    "                break\n",
    "            else:\n",
    "                agent.push_observation(old_state, action, reward, state, False)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print('Episode {:3d}: {:10.3f} iterations, eps={:6.6f}'.format(episode, \n",
    "                                                                           np.mean(len_mem[-100:]), \n",
    "                                                                           agent._eps))\n",
    "        agent.train_memory()\n",
    "        agent.memory.reset()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(len_mem, 'o', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "agent.eval()\n",
    "\n",
    "try:\n",
    "    env.seed = random.randint(0, 100000)\n",
    "    obs = env.reset()\n",
    "    env._max_episode_steps = 100000\n",
    "    for t in count():\n",
    "        \n",
    "        if t % 500 == 0:\n",
    "            print(t)\n",
    "        \n",
    "        env.render()\n",
    "        action = agent.get_action(obs, deterministic=True)\n",
    "#         print(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            env.close()\n",
    "            break\n",
    "\n",
    "    print(t)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()\n",
    "    print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qnet(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Qnet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'model.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = torch.load('model.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qnet(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_net = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
