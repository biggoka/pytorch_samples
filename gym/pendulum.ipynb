{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from itertools import count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(1,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(2,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.50980259,  0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.close()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for i in range(50):\n",
    "    env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.short_memory = []\n",
    "        \n",
    "    def push(self, old_state, action, reward, new_state, final):\n",
    "        self.short_memory.append([old_state, action, reward, new_state, final])\n",
    "        \n",
    "    def commit(self, final=True): \n",
    "        \n",
    "#         R = self.short_memory[-1][2]\n",
    "#         for i in range(len(self.short_memory) - 1):\n",
    "#             R = self.short_memory[-2 -i][2] + R*0.99\n",
    "#             self.short_memory[-2 -i][2] = R\n",
    "        \n",
    "        if len(self.memory) + len(self.short_memory) < self.capacity:\n",
    "            self.memory.extend(self.short_memory)\n",
    "        else:\n",
    "            for memo in self.short_memory:\n",
    "                self.memory.insert(random.randint(0, len(self.memory) - 1), memo)\n",
    "                \n",
    "        self.memory = self.memory[:self.capacity]\n",
    "        self.short_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor_net, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class Qritic_net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qritic_net, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 64),\n",
    "#             torch.nn.LeakyReLU(inplace=True),\n",
    "#             torch.nn.Linear(32, 32),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64 + 1, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, actions):\n",
    "        out = self.model(state)\n",
    "        out = torch.cat((out, actions), dim=1)\n",
    "        out = self.head(out)\n",
    "        return out\n",
    "\n",
    "policy_actor, policy_critic = Actor_net().to(device), Qritic_net().to(device)\n",
    "target_actor, target_critic = Actor_net().to(device), Qritic_net().to(device)\n",
    "\n",
    "target_actor.load_state_dict(policy_actor.state_dict())\n",
    "target_critic.load_state_dict(policy_critic.state_dict())\n",
    "\n",
    "critic_optim = torch.optim.Adam(policy_critic.parameters(), lr=0.01)\n",
    "actor_optim = torch.optim.Adam(policy_actor.parameters(), lr=0.01)\n",
    "# optim = torch.optim.SGD(policy_net.parameters(), lr=0.001)\n",
    "\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# scheduler = StepLR(optim, step_size=100, gamma=0.84)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   0:    199.000 iterations, eps=0.998800\n",
      "Episode  10:    199.000 iterations, eps=0.996805\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('CartPole-v0')\n",
    "# env._max_episode_steps = 500\n",
    "\n",
    "EPS = 0.999\n",
    "EPS_MIN = 0.05\n",
    "GAMMA = 0.999999\n",
    "\n",
    "\n",
    "def get_model_action(model, state, device, train=True):\n",
    "    global EPS, EPS_MIN, GAMMA\n",
    "    EPS = EPS * GAMMA\n",
    "    EPS = max(EPS, EPS_MIN)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(torch.FloatTensor(state).to(device).unsqueeze(0)).cpu().detach().numpy()\n",
    "        action = model_output[0]\n",
    "#             print(state, model_output, action)\n",
    "            \n",
    "    if train:\n",
    "        noise = (random.random() * 2) -1\n",
    "        noise = noise * EPS * 2.5\n",
    "        action = action + noise\n",
    "        action = np.clip(action, -1, 1)\n",
    "        \n",
    "        \n",
    "    return action\n",
    "\n",
    "def train(memory, device):\n",
    "    memory = memory.memory\n",
    "    try:\n",
    "        memory = random.sample(memory, 128)\n",
    "    except ValueError:\n",
    "        return\n",
    "\n",
    "    s0 = [x[0] for x in memory]\n",
    "    a = [x[1] for x in memory]\n",
    "    r = [[x[2]] for x in memory]\n",
    "    s1 = [x[3] for x in memory]\n",
    "    done = [1. if x[4] else 0. for x in memory]\n",
    "\n",
    "\n",
    "    s0 = torch.FloatTensor(s0).to(device)\n",
    "    s1 = torch.FloatTensor(s1).to(device)\n",
    "    a = torch.FloatTensor(a).to(device)\n",
    "    r = torch.FloatTensor(r).to(device)\n",
    "    done = torch.FloatTensor(done).to(device)\n",
    "\n",
    "    \n",
    "    y = target_critic(s1, target_actor(s1)).mul(0.99).add(r)\n",
    "    target = policy_critic(s0, a)\n",
    "#     loss = (y.detach() - target).view(-1).pow(2).mean()\n",
    "    loss = torch.nn.functional.smooth_l1_loss(y.detach(), target)\n",
    "    \n",
    "#     print('s0', s0)\n",
    "#     print('a', a)\n",
    "#     print('r', r)\n",
    "#     print('s1', s1)\n",
    "    \n",
    "#     print('y', y)\n",
    "#     print('target', target)\n",
    "#     print('loss', loss.item())\n",
    "#     assert False\n",
    "    \n",
    "    critic_optim.zero_grad()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_critic.parameters(), 1)\n",
    "    loss.backward()\n",
    "    critic_optim.step()\n",
    "    \n",
    "    policy_actions = policy_actor(s0)\n",
    "    q_values = policy_critic(s0, policy_actions)\n",
    "    \n",
    "    loss = q_values.mean().mul(-1)\n",
    "    actor_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    actor_optim.step()\n",
    "    \n",
    "#     print('policy_actions', policy_actions)\n",
    "#     print('q_values', q_values)\n",
    "#     print('loss', loss.item())\n",
    "#     assert False\n",
    "    \n",
    "    def soft_update(target_model, local_model, tau=0.1):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "                target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "                \n",
    "    soft_update(target_actor, policy_actor)\n",
    "    soft_update(target_critic, policy_critic)\n",
    "        \n",
    "#     scheduler.step()\n",
    "\n",
    "memory = Memory(capacity=100000)  \n",
    "len_mem = []\n",
    "cum_reward_mem = []\n",
    "\n",
    "try:\n",
    "    for episode in range(100000):\n",
    "        state = env.reset()\n",
    "\n",
    "        cum_reward = []\n",
    "        for i in count():\n",
    "#             env.render()\n",
    "\n",
    "            action = get_model_action(policy_actor, state, device)\n",
    "            old_state = state\n",
    "            state, reward, done, info = env.step(action)\n",
    "            \n",
    "#             reward = max(-0.1, state[0])\n",
    "#             reward *= 100\n",
    "\n",
    "            cum_reward.append(reward)\n",
    "\n",
    "            \n",
    "            if not done:\n",
    "                memory.push(old_state, action, reward, state, False)\n",
    "            else:\n",
    "                env.close()\n",
    "                len_mem.append(i)\n",
    "                cum_reward_mem.append(np.sum(cum_reward))\n",
    "\n",
    "#                 if state[0] >= 0.59:\n",
    "#                     reward = 100\n",
    "#                     final = False\n",
    "#                 else:\n",
    "#                     final = True\n",
    "                \n",
    "                memory.push(old_state, action, reward, state, True)\n",
    "                memory.commit()\n",
    "                break\n",
    "\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print('Episode {:3d}: {:10.3f} iterations, eps={:6.6f}'.format(episode, np.mean(len_mem[-100:]), EPS))\n",
    "        \n",
    "        train(memory, device)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f056b05b278>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAADmBJREFUeJzt3X+s3XV9x/Hny1bEoeGHvWu6tq64NRrmRiE3CNEsqJkCMQMTQyDL7AxJ9wcmGo1b2ZK5/bHExUyniSNjk4mJU/EXNIaArJr4l+itIBawoyoN7Qq9KqJRowLv/XE+xbPa9v449/Tc8/H5SE7O9/s533PO+5bDs6ffe26bqkKS1K/nTHoASdJ4GXpJ6pyhl6TOGXpJ6pyhl6TOGXpJ6pyhl6TOGXpJ6pyhl6TOrZ30AADr1q2rLVu2THoMSZoqe/bs+V5VzSx03KoI/ZYtW5ibm5v0GJI0VZIcWMxxnrqRpM4ZeknqnKGXpM4Zeknq3IKhT7I5yZeSPJjkgSRva+vnJLk7ycPt+uy2niQfTLI/yf1JLhz3FyFJOrHFfOrmKeCdVfX1JC8E9iS5G/gLYHdVvSfJTmAn8NfA5cDWdnkFcGO7XlG33XuIGz57Pz/75TMr/dCSdEqdcdoa/vGNf8hVF2wcy+Mv+I6+qg5X1dfb9o+Bh4CNwJXALe2wW4Cr2vaVwEdr4CvAWUk2rOTQt917iHd88j4jL6kLP/nF07zzU9/gtnsPjeXxl3SOPskW4ALgHmB9VR1uNz0GrG/bG4FHh+52sK0d+1g7kswlmZufn1/S0O+9ax8mXlJPnn6meO9d+8by2IsOfZIXAJ8B3l5VPxq+rQb/8OyS/vHZqrqpqmaranZmZsEf7Pp//veHP1vS8ZI0DcbVtkWFPslzGUT+Y1X12bb8+NFTMu36SFs/BGweuvumtrZifues56/kw0nSqjCuti3mUzcBPgw8VFXvG7ppF7C9bW8Hbh9af3P79M3FwJNDp3hWxLte/1I/FyqpK2ueE971+peO5bEX86mbVwJ/DnwzyX1t7W+A9wC3JrkOOABc3W67A7gC2A/8FHjLik4Mz35n2k/dSOrBuD91k8Hp9cmanZ0t/1IzSVqaJHuqanah4zwDIkmdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1LkFQ5/k5iRHkuwdWvv7JIeS3NcuVwzddkOS/Un2JXn9uAaXJC3OYt7RfwS47Djr76+qbe1yB0CS84BrgD9o9/nXJGtWalhJ0tItGPqq+jLwg0U+3pXAJ6rq51X1XWA/cNEI80mSRjTKOfq3Jrm/ndo5u61tBB4dOuZgW/s1SXYkmUsyNz8/P8IYkqSTWW7obwR+D9gGHAb+eakPUFU3VdVsVc3OzMwscwxJ0kKWFfqqeryqnq6qZ4B/51enZw4Bm4cO3dTWJEkTsqzQJ9kwtPtG4OgncnYB1yR5XpJzga3AV0cbUZI0irULHZDk48ClwLokB4F3A5cm2QYU8AjwlwBV9UCSW4EHgaeA66vq6fGMLklajFTVpGdgdna25ubmJj2GJE2VJHuqanah4/zJWEnqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4ZeknqnKGXpM4tGPokNyc5kmTv0No5Se5O8nC7PrutJ8kHk+xPcn+SC8c5vCRpYYt5R/8R4LJj1nYCu6tqK7C77QNcDmxtlx3AjSszpiRpuRYMfVV9GfjBMctXAre07VuAq4bWP1oDXwHOSrJhpYaVJC3dcs/Rr6+qw237MWB9294IPDp03MG2JkmakJG/GVtVBdRS75dkR5K5JHPz8/OjjiFJOoHlhv7xo6dk2vWRtn4I2Dx03Ka29muq6qaqmq2q2ZmZmWWOIUlayHJDvwvY3ra3A7cPrb+5ffrmYuDJoVM8kqQJWLvQAUk+DlwKrEtyEHg38B7g1iTXAQeAq9vhdwBXAPuBnwJvGcPMkqQlWDD0VXXtCW567XGOLeD6UYeSJK0cfzJWkjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc4Zekjpn6CWpc2tHuXOSR4AfA08DT1XVbJJzgE8CW4BHgKur6onRxpQkLddKvKN/dVVtq6rZtr8T2F1VW4HdbV+SNCHjOHVzJXBL274FuGoMzyFJWqRRQ1/AF5LsSbKjra2vqsNt+zFg/YjPIUkawUjn6IFXVdWhJL8N3J3kW8M3VlUlqePdsf3GsAPgxS9+8YhjSJJOZKR39FV1qF0fAT4HXAQ8nmQDQLs+coL73lRVs1U1OzMzM8oYkqSTWHbok5yR5IVHt4HXAXuBXcD2dth24PZRh5QkLd8op27WA59LcvRx/quq7kzyNeDWJNcBB4CrRx9TkrRcyw59VX0HOP84698HXjvKUJKkleNPxkpS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHXO0EtS5wy9JHVubKFPclmSfUn2J9k5rueRJJ3cWEKfZA3wIeBy4Dzg2iTnjeO5JEknN6539BcB+6vqO1X1C+ATwJVjei5J0kmMK/QbgUeH9g+2NUnSKTaxb8Ym2ZFkLsnc/Pz8pMaQpO6NK/SHgM1D+5va2rOq6qaqmq2q2ZmZmTGNIUkaV+i/BmxNcm6S04BrgF1jei5J0kmsHceDVtVTSd4K3AWsAW6uqgfG8VySpJMbS+gBquoO4I5xPb4kaXH8yVhJ6pyhl6TOGXpJ6pyhl6TOGXpJ6pyhl6TOGXpJ6pyhl6TOGXpJ6pyhl6TOGXpJ6pyhl6TOje0vNTsl7rwT3vGOU/+8Vavr+FPxHB7v8av5eIADB+D005d+v98A0x36M8+El7989MepgmRp91ltx5+K5/B4j1/Nx69Zs7Tjf4NMd+gvuWRwkSSdkOfoJalzhl6SOmfoJalzhl6SOmfoJalzhl6SOmfoJalzhl6SOpdazo8ar/QQyTxwYJl3Xwd8bwXHOZWcfTKcfTKcfeX9blXNLHTQqgj9KJLMVdXspOdYDmefDGefDGefHE/dSFLnDL0kda6H0N806QFG4OyT4eyT4ewTMvXn6CVJJ9fDO3pJ0klMdeiTXJZkX5L9SXZOep5jJbk5yZEke4fWzklyd5KH2/XZbT1JPti+lvuTXDi5ySHJ5iRfSvJgkgeSvG1a5k9yepKvJvlGm/0f2vq5Se5pM34yyWlt/Xltf3+7fcukZm/zrElyb5LPT9PcbaZHknwzyX1J5traqn/NtHnOSvLpJN9K8lCSS6Zl9oVMbeiTrAE+BFwOnAdcm+S8yU71az4CXHbM2k5gd1VtBXa3fRh8HVvbZQdw4yma8USeAt5ZVecBFwPXt1/faZj/58Brqup8YBtwWZKLgX8C3l9Vvw88AVzXjr8OeKKtv78dN0lvAx4a2p+WuY96dVVtG/o44jS8ZgA+ANxZVS8Dzmfw32BaZj+5qprKC3AJcNfQ/g3ADZOe6zhzbgH2Du3vAza07Q3Avrb9b8C1xztuNVyA24E/mbb5gd8Cvg68gsEPvKw99vUD3AVc0rbXtuMyoXk3MQjKa4DPA5mGuYfmfwRYd8zaqn/NAGcC3z32128aZl/MZWrf0QMbgUeH9g+2tdVufVUdbtuPAevb9qr9etopgQuAe5iS+dvpj/uAI8DdwLeBH1bVU8eZ79nZ2+1PAi86tRM/61+AvwKeafsvYjrmPqqALyTZk2RHW5uG18y5wDzwn+202X8kOYPpmH1B0xz6qVeDtwKr+mNPSV4AfAZ4e1X9aPi21Tx/VT1dVdsYvEO+CHjZhEdaUJI3AEeqas+kZxnBq6rqQganNq5P8sfDN67i18xa4ELgxqq6APgJvzpNA6zq2Rc0zaE/BGwe2t/U1la7x5NsAGjXR9r6qvt6kjyXQeQ/VlWfbctTMz9AVf0Q+BKDUx5nJVnbbhqe79nZ2+1nAt8/xaMCvBL40ySPAJ9gcPrmA6z+uZ9VVYfa9RHgcwx+k52G18xB4GBV3dP2P80g/NMw+4KmOfRfA7a2TyScBlwD7JrwTIuxC9jetrczOPd9dP3N7bv5FwNPDv2R8ZRLEuDDwENV9b6hm1b9/ElmkpzVtp/P4HsLDzEI/pvaYcfOfvRrehPwxfbu7ZSqqhuqalNVbWHwev5iVf0Zq3zuo5KckeSFR7eB1wF7mYLXTFU9Bjya5KVt6bXAg0zB7Isy6W8SjHIBrgD+h8H517+d9DzHme/jwGHglwzeMVzH4BzqbuBh4L+Bc9qxYfApom8D3wRmJzz7qxj8MfV+4L52uWIa5gf+CLi3zb4X+Lu2/hLgq8B+4FPA89r66W1/f7v9JavgtXMp8PlpmrvN+Y12eeDo/5PT8Jpp82wD5trr5jbg7GmZfaGLPxkrSZ2b5lM3kqRFMPSS1DlDL0mdM/SS1DlDL0mdM/SS1DlDL0mdM/SS1Ln/A0qpFHSFIG/hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05507c2f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(len_mem, 'o')\n",
    "plt.plot(pd.Series(cum_reward_mem).rolling(100).mean().dropna().values, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "[1.] -0.1\n",
      "199\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "#     env.seed = random.randint(0, 100000)\n",
    "    obs = env.reset()\n",
    "    env._max_episode_steps = 200\n",
    "    for t in count():\n",
    "        \n",
    "        if t % 500 == 0:\n",
    "            print(t)\n",
    "        \n",
    "        env.render()\n",
    "        global EPS\n",
    "        EPS = 0.95\n",
    "        action = get_model_action(policy_actor, obs, device, train=False)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print(action, reward)\n",
    "        if done:\n",
    "            env.close()\n",
    "            break\n",
    "\n",
    "    print(t)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qnet(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Qnet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'cart.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = torch.load('cart.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qnet(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_net = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
